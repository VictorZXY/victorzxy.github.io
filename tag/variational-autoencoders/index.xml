<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Variational Autoencoders | Xiangyu Zhao</title>
    <link>https://victorzxy.github.io/tag/variational-autoencoders/</link>
      <atom:link href="https://victorzxy.github.io/tag/variational-autoencoders/index.xml" rel="self" type="application/rss+xml" />
    <description>Variational Autoencoders</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2022 Xiangyu Zhao</copyright><lastBuildDate>Wed, 19 Jan 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://victorzxy.github.io/media/icon_hu51ffffd5f6381cda320d0c97647d90a4_34098_512x512_fill_lanczos_center_3.png</url>
      <title>Variational Autoencoders</title>
      <link>https://victorzxy.github.io/tag/variational-autoencoders/</link>
    </image>
    
    <item>
      <title>Function Autoencoders: A Neural Network Approach to Gaussian Processes</title>
      <link>https://victorzxy.github.io/post/function-autoencoders/</link>
      <pubDate>Wed, 19 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://victorzxy.github.io/post/function-autoencoders/</guid>
      <description>&lt;p&gt;Gaussian processes (GPs) are data-efficient and flexible probabilistic methods that learn distributions of functions based on given priors. However, GPs suffer from unscalability as they become very computationally expensive on large datasets, and choosing the appropriate priors for GPs can be nontrivial. In this project, I investigated a neural network (NN) alternative to GPs, and introduced the function autoencoders that preserve GPs’ own advantages and avoid their weaknesses with NNs’ benefits. I tested the performance of the various function autoencoders on a 1-dimensional function regression task, using random functions generated by a GP with varying kernel parameters, and compared and analysed their results. The trained function autoencoder models indeed have the ability to learn distributions over random functions, and performed decently on the selected task. Moreover, the function autoencoders demonstrate a great potential for further improvements.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Function Autoencoders: A Neural Network Approach to Gaussian Processes</title>
      <link>https://victorzxy.github.io/project/function-autoencoders/</link>
      <pubDate>Wed, 19 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://victorzxy.github.io/project/function-autoencoders/</guid>
      <description>&lt;p&gt;Gaussian processes (GPs) are data-efficient and flexible probabilistic methods that learn distributions of functions based on given priors. However, GPs suffer from unscalability as they become very computationally expensive on large datasets, and choosing the appropriate priors for GPs can be nontrivial. In this project, I investigated a neural network (NN) alternative to GPs, and introduced the function autoencoders that preserve GPs’ own advantages and avoid their weaknesses with NNs’ benefits. I tested the performance of the various function autoencoders on a 1-dimensional function regression task, using random functions generated by a GP with varying kernel parameters, and compared and analysed their results. The trained function autoencoder models indeed have the ability to learn distributions over random functions, and performed decently on the selected task. Moreover, the function autoencoders demonstrate a great potential for further improvements.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
