<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Reinforcement Learning | Xiangyu Zhao</title>
    <link>https://victorzxy.github.io/tag/reinforcement-learning/</link>
      <atom:link href="https://victorzxy.github.io/tag/reinforcement-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Reinforcement Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2022 Xiangyu Zhao</copyright><lastBuildDate>Thu, 17 Mar 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://victorzxy.github.io/media/icon_hu51ffffd5f6381cda320d0c97647d90a4_34098_512x512_fill_lanczos_center_3.png</url>
      <title>Reinforcement Learning</title>
      <link>https://victorzxy.github.io/tag/reinforcement-learning/</link>
    </image>
    
    <item>
      <title>Multi-Agent Deep Q-Learning for the Berry Poisoning Game</title>
      <link>https://victorzxy.github.io/post/dqn-berry-poisoning/</link>
      <pubDate>Thu, 17 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://victorzxy.github.io/post/dqn-berry-poisoning/</guid>
      <description>&lt;p&gt;Deep Q-learning (DQN) is a successful algorithm that combines deep learning with reinforcement learning. However, it is of great research interest whether this method can work in a multi-agent environment. In this mini-project, I performed a multi-agent DQN method on the Berry Poisoning Games, and investigated on the agent performance with respect to different game environment parameters, including the bad berry rate, bad/good berry reward ratio, number of agents, and agent visibility range. The results clearly show that my DQN method can successfully train agents to act sensibly in such an environment, within only a few episodes of training. My DQN method also succeeds in transfer learning, training agents that still perform well in other game environment setups, and can be further enhanced through fine-tuning.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multi-Agent Deep Q-Learning for the Berry Poisoning Game</title>
      <link>https://victorzxy.github.io/project/dqn-berry-poisoning/</link>
      <pubDate>Thu, 17 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://victorzxy.github.io/project/dqn-berry-poisoning/</guid>
      <description>&lt;p&gt;Deep Q-learning (DQN) is a successful algorithm that combines deep learning with reinforcement learning. However, it is of great research interest whether this method can work in a multi-agent environment. In this mini-project, I performed a multi-agent DQN method on the Berry Poisoning Games, and investigated on the agent performance with respect to different game environment parameters, including the bad berry rate, bad/good berry reward ratio, number of agents, and agent visibility range. The results clearly show that my DQN method can successfully train agents to act sensibly in such an environment, within only a few episodes of training. My DQN method also succeeds in transfer learning, training agents that still perform well in other game environment setups, and can be further enhanced through fine-tuning.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Building a 3-Player Mahjong AI using Deep Reinforcement Learning</title>
      <link>https://victorzxy.github.io/publication/2022-meowjong/</link>
      <pubDate>Fri, 25 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://victorzxy.github.io/publication/2022-meowjong/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Asynchronous Methods for Deep Reinforcement Learning</title>
      <link>https://victorzxy.github.io/talk/asynchronous-methods-for-deep-reinforcement-learning/</link>
      <pubDate>Fri, 11 Feb 2022 15:00:00 +0000</pubDate>
      <guid>https://victorzxy.github.io/talk/asynchronous-methods-for-deep-reinforcement-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Deep Reinforcement Learning for Mahjong</title>
      <link>https://victorzxy.github.io/post/meowjong/</link>
      <pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate>
      <guid>https://victorzxy.github.io/post/meowjong/</guid>
      <description>&lt;p&gt;Mahjong is a popular multi-player imperfect-information game developed in China in the late 19th-century, with some very challenging features for AI research. Sanma, being a 3-player variant of the Japanese Riichi Mahjong, possesses unique characteristics including fewer tiles and, consequently, a more aggressive playing style. It is thus challenging and of great research interest in its own right, but has not yet been explored. In this project, I built Meowjong, an AI for Sanma using deep reinforcement learning. I defined an informative and compact 2-dimensional data structure for encoding the observable information in a Sanma game. Then, I pre-trained 5 convolutional neural networks (CNNs) for Sanma&amp;rsquo;s 5 actions—discard, Pon, Kan, Kita and Riichi, and enhanced the major action&amp;rsquo;s model, namely the discard model, via self-play reinforcement learning using the Monte Carlo policy gradient method. I have also implemeted necessary additional modules including data collection and processing, Mahjong hand calculation and a game simulator, for the agents to be trained and evaluated. Meowjong&amp;rsquo;s models have achieved test accuracies comparable with AIs for 4-player Mahjong through supervised learning, and gained a significant further enhancement from reinforcement learning. Being the first ever AI in Sanma, Meowjong stands as a state-of-the-art in this game.&lt;/p&gt;
&lt;p&gt;A paper from this dissertation has been submitted to IEEE Conference on Games (IEEE CoG) 2022.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Reinforcement Learning for Mahjong</title>
      <link>https://victorzxy.github.io/project/meowjong/</link>
      <pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate>
      <guid>https://victorzxy.github.io/project/meowjong/</guid>
      <description>&lt;p&gt;Mahjong is a popular multi-player imperfect-information game developed in China in the late 19th-century, with some very challenging features for AI research. Sanma, being a 3-player variant of the Japanese Riichi Mahjong, possesses unique characteristics including fewer tiles and, consequently, a more aggressive playing style. It is thus challenging and of great research interest in its own right, but has not yet been explored. In this project, I built Meowjong, an AI for Sanma using deep reinforcement learning. I defined an informative and compact 2-dimensional data structure for encoding the observable information in a Sanma game. Then, I pre-trained 5 convolutional neural networks (CNNs) for Sanma&amp;rsquo;s 5 actions—discard, Pon, Kan, Kita and Riichi, and enhanced the major action&amp;rsquo;s model, namely the discard model, via self-play reinforcement learning using the Monte Carlo policy gradient method. I have also implemeted necessary additional modules including data collection and processing, Mahjong hand calculation and a game simulator, for the agents to be trained and evaluated. Meowjong&amp;rsquo;s models have achieved test accuracies comparable with AIs for 4-player Mahjong through supervised learning, and gained a significant further enhancement from reinforcement learning. Being the first ever AI in Sanma, Meowjong stands as a state-of-the-art in this game.&lt;/p&gt;
&lt;p&gt;A paper from this dissertation has been submitted to IEEE Conference on Games (IEEE CoG) 2022.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
