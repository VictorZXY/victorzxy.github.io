<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>PyTorch | Xiangyu Zhao</title>
    <link>https://victorzxy.github.io/tag/pytorch/</link>
      <atom:link href="https://victorzxy.github.io/tag/pytorch/index.xml" rel="self" type="application/rss+xml" />
    <description>PyTorch</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2022 Xiangyu Zhao</copyright><lastBuildDate>Thu, 17 Mar 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://victorzxy.github.io/media/icon_hu51ffffd5f6381cda320d0c97647d90a4_34098_512x512_fill_lanczos_center_3.png</url>
      <title>PyTorch</title>
      <link>https://victorzxy.github.io/tag/pytorch/</link>
    </image>
    
    <item>
      <title>Multi-Agent Deep Q-Learning for the Berry Poisoning Game</title>
      <link>https://victorzxy.github.io/post/dqn-berry-poisoning/</link>
      <pubDate>Thu, 17 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://victorzxy.github.io/post/dqn-berry-poisoning/</guid>
      <description>&lt;p&gt;Deep Q-learning (DQN) is a successful algorithm that combines deep learning with reinforcement learning. However, it is of great research interest whether this method can work in a multi-agent environment. In this mini-project, I performed a multi-agent DQN method on the Berry Poisoning Games, and investigated on the agent performance with respect to different game environment parameters, including the bad berry rate, bad/good berry reward ratio, number of agents, and agent visibility range. The results clearly show that my DQN method can successfully train agents to act sensibly in such an environment, within only a few episodes of training. My DQN method also succeeds in transfer learning, training agents that still perform well in other game environment setups, and can be further enhanced through fine-tuning.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multi-Agent Deep Q-Learning for the Berry Poisoning Game</title>
      <link>https://victorzxy.github.io/project/dqn-berry-poisoning/</link>
      <pubDate>Thu, 17 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://victorzxy.github.io/project/dqn-berry-poisoning/</guid>
      <description>&lt;p&gt;Deep Q-learning (DQN) is a successful algorithm that combines deep learning with reinforcement learning. However, it is of great research interest whether this method can work in a multi-agent environment. In this mini-project, I performed a multi-agent DQN method on the Berry Poisoning Games, and investigated on the agent performance with respect to different game environment parameters, including the bad berry rate, bad/good berry reward ratio, number of agents, and agent visibility range. The results clearly show that my DQN method can successfully train agents to act sensibly in such an environment, within only a few episodes of training. My DQN method also succeeds in transfer learning, training agents that still perform well in other game environment setups, and can be further enhanced through fine-tuning.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Function Autoencoders: A Neural Network Approach to Gaussian Processes</title>
      <link>https://victorzxy.github.io/post/function-autoencoders/</link>
      <pubDate>Wed, 19 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://victorzxy.github.io/post/function-autoencoders/</guid>
      <description>&lt;p&gt;Gaussian processes (GPs) are data-efficient and flexible probabilistic methods that learn distributions of functions based on given priors. However, GPs suffer from unscalability as they become very computationally expensive on large datasets, and choosing the appropriate priors for GPs can be nontrivial. In this project, I investigated a neural network (NN) alternative to GPs, and introduced the function autoencoders that preserve GPs’ own advantages and avoid their weaknesses with NNs’ benefits. I tested the performance of the various function autoencoders on a 1-dimensional function regression task, using random functions generated by a GP with varying kernel parameters, and compared and analysed their results. The trained function autoencoder models indeed have the ability to learn distributions over random functions, and performed decently on the selected task. Moreover, the function autoencoders demonstrate a great potential for further improvements.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Function Autoencoders: A Neural Network Approach to Gaussian Processes</title>
      <link>https://victorzxy.github.io/project/function-autoencoders/</link>
      <pubDate>Wed, 19 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://victorzxy.github.io/project/function-autoencoders/</guid>
      <description>&lt;p&gt;Gaussian processes (GPs) are data-efficient and flexible probabilistic methods that learn distributions of functions based on given priors. However, GPs suffer from unscalability as they become very computationally expensive on large datasets, and choosing the appropriate priors for GPs can be nontrivial. In this project, I investigated a neural network (NN) alternative to GPs, and introduced the function autoencoders that preserve GPs’ own advantages and avoid their weaknesses with NNs’ benefits. I tested the performance of the various function autoencoders on a 1-dimensional function regression task, using random functions generated by a GP with varying kernel parameters, and compared and analysed their results. The trained function autoencoder models indeed have the ability to learn distributions over random functions, and performed decently on the selected task. Moreover, the function autoencoders demonstrate a great potential for further improvements.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Investigating Adversarial Examples for Deep Residual Networks</title>
      <link>https://victorzxy.github.io/post/fgsm-attack-resnets/</link>
      <pubDate>Fri, 19 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://victorzxy.github.io/post/fgsm-attack-resnets/</guid>
      <description>&lt;p&gt;In this project, I investigated the one-shot and iterative fast gradient sign method (FGSM) attack, both untargeted and targeted, to generate adversarial examples for ResNet-50, and experimented the transferability of each FGSM to other neural networks, namely ResNet-18 and VGG-19, as well as other image inputs. The following conclusions can be reached after the above experiments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Targeted FGSM is more effective than FGSM;&lt;/li&gt;
&lt;li&gt;In white-box attacks, both the untargeted and targeted one-shot FGSMs are less effective than the iterative methods (both untargeted and targeted);&lt;/li&gt;
&lt;li&gt;When it comes to transferability to other neural networks or inputs, the one-shot FGSMs turn out to be much more effective than the iterative FGSMs, as the iterative FGSMs suffer from overfitting and hence completely unable to transfer to other neural networks or inputs.&lt;/li&gt;
&lt;li&gt;Since transferability is a foundation to black-box attacks, it is likely that the one-shot FGSMs can also be more effective than the iterative FGSMs in black-box attacks.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Investigating Adversarial Examples for Deep Residual Networks</title>
      <link>https://victorzxy.github.io/project/fgsm-attack-resnets/</link>
      <pubDate>Fri, 19 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://victorzxy.github.io/project/fgsm-attack-resnets/</guid>
      <description>&lt;p&gt;In this project, I investigated the one-shot and iterative fast gradient sign method (FGSM) attack, both untargeted and targeted, to generate adversarial examples for ResNet-50, and experimented the transferability of each FGSM to other neural networks, namely ResNet-18 and VGG-19, as well as other image inputs. The following conclusions can be reached after the above experiments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Targeted FGSM is more effective than FGSM;&lt;/li&gt;
&lt;li&gt;In white-box attacks, both the untargeted and targeted one-shot FGSMs are less effective than the iterative methods (both untargeted and targeted);&lt;/li&gt;
&lt;li&gt;When it comes to transferability to other neural networks or inputs, the one-shot FGSMs turn out to be much more effective than the iterative FGSMs, as the iterative FGSMs suffer from overfitting and hence completely unable to transfer to other neural networks or inputs.&lt;/li&gt;
&lt;li&gt;Since transferability is a foundation to black-box attacks, it is likely that the one-shot FGSMs can also be more effective than the iterative FGSMs in black-box attacks.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
